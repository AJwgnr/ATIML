{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and constants \n",
    "################################################################\n",
    "\n",
    "import csv\n",
    "import graphviz\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize, Imputer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#Print options for printing the pandas dataframe in the console\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#set numpy random seed for reproducible results\n",
    "np.random.seed(1234)\n",
    "#to view the whole output in the jupyter console (not only an extract)\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "#Data parameter\n",
    "#################################################################\n",
    "#File paths\n",
    "TEST_PATH ='dataset/aps_failure_test_set.csv'\n",
    "TRAIN_PATH ='dataset/aps_failure_training_set.csv'\n",
    "\n",
    "#Columns of the readed dataframe\n",
    "COLUMNS = ['aa_000', 'ab_000', 'ac_000', 'ad_000', 'ae_000', 'af_000', 'ag_000', 'ag_001', 'ag_002', 'ag_003', 'ag_004', 'ag_005', 'ag_006', 'ag_007', 'ag_008', 'ag_009', 'ah_000', 'ai_000', 'aj_000', 'ak_000', 'al_000', 'am_0', 'an_000', 'ao_000', 'ap_000', 'aq_000', 'ar_000', 'as_000', 'at_000', 'au_000', 'av_000', 'ax_000', 'ay_000', 'ay_001', 'ay_002', 'ay_003', 'ay_004', 'ay_005', 'ay_006', 'ay_007', 'ay_008', 'ay_009', 'az_000', 'az_001', 'az_002', 'az_003', 'az_004', 'az_005', 'az_006', 'az_007', 'az_008', 'az_009', 'ba_000', 'ba_001', 'ba_002', 'ba_003', 'ba_004', 'ba_005', 'ba_006', 'ba_007', 'ba_008', 'ba_009', 'bb_000', 'bc_000', 'bd_000', 'be_000', 'bf_000', 'bg_000', 'bh_000', 'bi_000', 'bj_000', 'bk_000', 'bl_000', 'bm_000', 'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'bs_000', 'bt_000', 'bu_000', 'bv_000', 'bx_000', 'by_000', 'bz_000', 'ca_000', 'cb_000', 'cc_000', 'cd_000', 'ce_000', 'cf_000', 'cg_000', 'ch_000', 'ci_000', 'cj_000', 'ck_000', 'cl_000', 'cm_000', 'cn_000', 'cn_001', 'cn_002', 'cn_003', 'cn_004', 'cn_005', 'cn_006', 'cn_007', 'cn_008', 'cn_009', 'co_000', 'cp_000', 'cq_000', 'cr_000', 'cs_000', 'cs_001', 'cs_002', 'cs_003', 'cs_004', 'cs_005', 'cs_006', 'cs_007', 'cs_008', 'cs_009', 'ct_000', 'cu_000', 'cv_000', 'cx_000', 'cy_000', 'cz_000', 'da_000', 'db_000', 'dc_000', 'dd_000', 'de_000', 'df_000', 'dg_000', 'dh_000', 'di_000', 'dj_000', 'dk_000', 'dl_000', 'dm_000', 'dn_000', 'do_000', 'dp_000', 'dq_000', 'dr_000', 'ds_000', 'dt_000', 'du_000', 'dv_000', 'dx_000', 'dy_000', 'dz_000', 'ea_000', 'eb_000', 'ec_00', 'ed_000', 'ee_000', 'ee_001', 'ee_002', 'ee_003', 'ee_004', 'ee_005', 'ee_006', 'ee_007', 'ee_008', 'ee_009', 'ef_000', 'eg_000']\n",
    "#Columns of the the final dataframe with processed histograms\n",
    "HISTOGRAMMERGEDCOLUMNS = ['aa_000', 'ab_000', 'ac_000', 'ad_000', 'ae_000', 'af_000',\n",
    "                           'ah_000', 'ai_000',\n",
    "                          'aj_000', 'ak_000', 'al_000', 'am_0', 'an_000', 'ao_000', 'ap_000',\n",
    "                          'aq_000', 'ar_000', 'as_000', 'at_000', 'au_000', 'av_000', 'ax_000',\n",
    "                             'bb_000', 'bc_000', 'bd_000', 'be_000', 'bf_000', \n",
    "                          'bg_000', 'bh_000', 'bi_000', 'bj_000', 'bk_000', 'bl_000', 'bm_000', \n",
    "                          'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'bs_000', 'bt_000', \n",
    "                          'bu_000', 'bv_000', 'bx_000', 'by_000', 'bz_000', 'ca_000', 'cb_000', \n",
    "                          'cc_000', 'cd_000', 'ce_000', 'cf_000', 'cg_000', 'ch_000', 'ci_000', \n",
    "                          'cj_000', 'ck_000', 'cl_000', 'cm_000', \n",
    "                          'co_000', 'cp_000', 'cq_000', 'cr_000',  \n",
    "                          'ct_000', 'cu_000', 'cv_000', 'cx_000', 'cy_000', 'cz_000', 'da_000', \n",
    "                          'db_000', 'dc_000', 'dd_000', 'de_000', 'df_000', 'dg_000', 'dh_000', \n",
    "                          'di_000', 'dj_000', 'dk_000', 'dl_000', 'dm_000', 'dn_000', 'do_000', \n",
    "                          'dp_000', 'dq_000', 'dr_000', 'ds_000', 'dt_000', 'du_000', 'dv_000', \n",
    "                          'dx_000', 'dy_000', 'dz_000', 'ea_000', 'eb_000', 'ec_00', 'ed_000', \n",
    "                          'ef_000', 'eg_000','ag_median','ay_median', 'az_median', 'ba_median','cn_median','cs_median','ee_median']\n",
    "\n",
    "#possible Class Labels in the dataset\n",
    "CLASS_LABELS = ['neg', 'pos']\n",
    "#Parameter to skip the first [SKIPROWS] lines in the csv files before reading into the dataframe\n",
    "SKIPROWS = 20\n",
    "\n",
    "#Histogram columns\n",
    "AG = np.array(['ag_000', 'ag_001', 'ag_002', 'ag_003', 'ag_004', 'ag_005', 'ag_006', 'ag_007', 'ag_008', 'ag_009'])\n",
    "AY = np.array(['ay_000', 'ay_001', 'ay_002', 'ay_003', 'ay_004', 'ay_005', 'ay_006', 'ay_007', 'ay_008', 'ay_009'])\n",
    "AZ = np.array(['az_000', 'az_001', 'az_002', 'az_003', 'az_004', 'az_005', 'az_006', 'az_007', 'az_008', 'az_009'])\n",
    "BA = np.array(['ba_000', 'ba_001', 'ba_002', 'ba_003', 'ba_004', 'ba_005', 'ba_006', 'ba_007', 'ba_008', 'ba_009'])\n",
    "CN = np.array(['cn_000', 'cn_001', 'cn_002', 'cn_003', 'cn_004', 'cn_005', 'cn_006', 'cn_007', 'cn_008', 'cn_009'])\n",
    "CS = np.array(['cs_000', 'cs_001', 'cs_002', 'cs_003', 'cs_004', 'cs_005', 'cs_006', 'cs_007', 'cs_008', 'cs_009'])\n",
    "EE = np.array(['ee_000', 'ee_001', 'ee_002', 'ee_003', 'ee_004', 'ee_005', 'ee_006', 'ee_007', 'ee_008', 'ee_009'])\n",
    "\n",
    "#NAN Data preprocessing parameter\n",
    "NaNSTRATEGY = ['mean', 'median', 'most_frequent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Methods dealing with the data preprocessing \n",
    "##################################################################################\n",
    "\n",
    "#Reads the csv file from the given path as a pandas dataframe\n",
    "#returns the dataframe\n",
    "def loadDatasetWithPandas(path, skiprowsNum):\n",
    "    # Reading the raw data from csv file\n",
    "    rawData = pd.read_csv(path, skiprows=skiprowsNum)\n",
    "    # replacing the string indicating missing values with the numpy value for missing values\n",
    "    NaNProcessedData = rawData.replace({'na': np.nan}, regex=True)\n",
    "    return NaNProcessedData\n",
    "\n",
    "#creates a new dataframe from the given datafame where the different histogram bins are processed into a new feature.\n",
    "#returned dataframe is structured according to 'HISTOGRAMMERGEDCOLUMNS' variable\n",
    "def histogramProcessing(dataf):\n",
    "    data_set=dataf\n",
    "    ag_ = data_set[AG]\n",
    "    ay_ = data_set[AY]\n",
    "    az_ = data_set[AZ]\n",
    "    ba_ = data_set[BA]\n",
    "    cn_ = data_set[CN]\n",
    "    cs_ = data_set[CS]\n",
    "    ee_ = data_set[EE]\n",
    "    \n",
    "    # create new dataframe for each of the above with the mean\\n\",\n",
    "    ag_mean = ag_.mean(axis=1, skipna=True)\n",
    "    ay_mean = ay_.mean(axis=1, skipna=True)\n",
    "    az_mean = az_.mean(axis=1, skipna=True)\n",
    "    ba_mean = ba_.mean(axis=1, skipna=True)\n",
    "    cn_mean = cn_.mean(axis=1, skipna=True)\n",
    "    cs_mean = cs_.mean(axis=1, skipna=True)\n",
    "    ee_mean = ee_.mean(axis=1, skipna=True)\n",
    "\n",
    "    remaining_columns = np.setdiff1d(np.setdiff1d(np.setdiff1d(np.setdiff1d(np.setdiff1d(np.setdiff1d(np.setdiff1d(np.array(COLUMNS), AG), AY), AZ), BA), CN), CS), EE)\n",
    "    remaining_columns= np.concatenate([['class'],remaining_columns])\n",
    "    remaining_data = data_set[remaining_columns]\n",
    "    return pd.concat(\n",
    "        [\n",
    "            remaining_data,\n",
    "            ag_mean.rename('ag_median'),\n",
    "            ay_mean.rename('ay_median'),\n",
    "            az_mean.rename('az_median'),\n",
    "            ba_mean.rename('ba_median'),\n",
    "            cn_mean.rename('cn_median'),\n",
    "            cs_mean.rename('cs_median'),\n",
    "            ee_mean.rename('ee_median')\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "#processes the NaN values in data frame according to the provided strategy (mean, median or most_frequent) usinjg scikits Imputer.\n",
    "# returns a dataframe without any NaN values\n",
    "def processNaNInDataset(data, strategy):\n",
    "    values = data[list(COLUMNS)].values\n",
    "    imp = Imputer(missing_values='NaN', strategy=strategy, axis=0)\n",
    "    imp = imp.fit(values)\n",
    "    cleanedValues = imp.transform(values)\n",
    "    return cleanedValues\n",
    "\n",
    "#loads the csv and uses the 3 methods above to process file (loading, Nan processing)\n",
    "#returns the loaded testdatframe and trainingdataframe\n",
    "def loadDataset(strategy):\n",
    "    #Load Train Dataset\n",
    "    trainData = loadDatasetWithPandas(TRAIN_PATH, SKIPROWS)\n",
    "    #Load Test Dataset\n",
    "    testData = loadDatasetWithPandas(TEST_PATH, SKIPROWS)\n",
    "    #if no strategy for nan processing is provided just return the laoded dataframes\n",
    "    if(strategy == None):\n",
    "        return trainData, testData\n",
    "\n",
    "    #remove class column from datset to allow conversion to float datatype\n",
    "    removeClassTrain = trainData.iloc[:,1:171]\n",
    "    removeClassTest = testData.iloc[:,1:171]\n",
    "    # change datatype of the dataframe to allow computing of the mean of a column (otherwise an overflow will happen)\n",
    "    removeClassTrain = removeClassTrain.astype('float64')\n",
    "    processedTrain = processNaNInDataset(removeClassTrain,strategy)\n",
    "\n",
    "    removeClassTest = removeClassTest.astype('float64')\n",
    "    processedTest = processNaNInDataset(removeClassTest,strategy)\n",
    "   \n",
    "\n",
    "    finalTrainFrame = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(trainData.iloc[:,0]),\n",
    "                pd.DataFrame(processedTrain, columns=COLUMNS)\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    finalTestFrame = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(testData.iloc[:,0]),\n",
    "                pd.DataFrame(processedTest, columns=COLUMNS)\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "    return finalTrainFrame, finalTestFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All methods dealing with evaluation\n",
    "#################################################################################\n",
    "\n",
    "#computes/plots scikit classification report, confusion matrix and calculates the overall cost of the result using the formula provided in the dataset\n",
    "def evaluate(trueLabels, predictions):\n",
    "    classificationRep = classification_report(trueLabels, predictions)\n",
    "    print(classificationRep)\n",
    "    \n",
    "    confusionMatrix = confusion_matrix(trueLabels,predictions) \n",
    "    #print(confusionMatrix)\n",
    "    #np.set_printoptions(precision=2)\n",
    "    #Code for plotting Confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusionMatrix, classes=CLASS_LABELS,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusionMatrix, classes=CLASS_LABELS, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "    plt.show()\n",
    "    #####################\n",
    "    score = calculateOverallCostFromConfusionMatrix(confusionMatrix)\n",
    "    print('Score: ' + str(score))\n",
    "    return score\n",
    "    \n",
    "#calculates the overall cost from the given confusionMAtrix\n",
    "def calculateOverallCostFromConfusionMatrix(confusionMatrix):\n",
    "    #cost function from description\n",
    "    score = confusionMatrix[1][0] * 500 + 10*confusionMatrix[0][1]\n",
    "    return score\n",
    "\n",
    "#plots the roc curve for given true labels and the predicted results\n",
    "def plot_roc(true_lables, predictions):\n",
    "    true_labels_binarized = label_binarize(true_lables, classes=['neg', 'pos']).flatten()\n",
    "    predictions_binarized = label_binarize(predictions, classes=['neg', 'pos']).flatten()\n",
    "    false_positives, true_positives, thresholds = roc_curve(true_labels_binarized, predictions_binarized, pos_label=1)\n",
    "    roc_auc = auc(false_positives, true_positives)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(false_positives, true_positives, 'b',\n",
    "    label='AUC = %0.2f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.2])\n",
    "    plt.ylim([-0.1,1.2])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading of all datasets and possible imputations (thus data has to be loaded only once)\n",
    "####################################################################\n",
    "\n",
    "#Data, NAN not processed (rows with empty values removed)\n",
    "trainSet, testSet = loadDataset(None)\n",
    "trainSet.dropna(inplace=True)\n",
    "testSet.dropna(inplace=True)\n",
    "trainSet  = pd.DataFrame(trainSet)\n",
    "testSet  = pd.DataFrame(testSet)\n",
    "\n",
    "#Normal data NAN Processed (different strategys used)\n",
    "meanTrainSet, meanTestSet = loadDataset(NaNSTRATEGY[0])\n",
    "medianTrainSet, medianTestSet = loadDataset(NaNSTRATEGY[1])\n",
    "mostFrequentTrainSet, mostFrequentTestSet = loadDataset(NaNSTRATEGY[2])\n",
    "\n",
    "#Processed Histogram from normal, with NaN processed data\n",
    "meanTrainSetHist = histogramProcessing(meanTrainSet)\n",
    "meanTestSetHist = histogramProcessing(meanTestSet)\n",
    "medianTrainSetHist = histogramProcessing(medianTrainSet)\n",
    "medianTestSetHist = histogramProcessing(medianTestSet)\n",
    "mostFrequentTrainSetHist = histogramProcessing(mostFrequentTrainSet)\n",
    "mostFrequentTestSetHist = histogramProcessing(mostFrequentTestSet)"
     
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of scikit Decision Tree\n",
    "###########################################################\n",
    "\n",
    "max_depth = 100\n",
    "min_samples_leaf = 10\n",
    "\n",
    "#Use max_depth, and min_sample leafs to prevent overfitting according to http://scikit-learn.org/stable/modules/tree.html#tree 1.10.5\n",
    "\n",
    "'''\n",
    "#histogram bin as feature\n",
    "classifier = DecisionTreeClassifier()#class_weight={'pos':.983, 'neg':.017},max_depth =max_depth, min_samples_leaf =min_samples_leaf)  \n",
    "classifier.fit(pd.DataFrame(data=meanTrainSet,columns=COLUMNS), pd.DataFrame(data=meanTrainSet,columns=['class']))  \n",
    "predictions = classifier.predict(pd.DataFrame(data=meanTestSet,columns=COLUMNS))  \n",
    "score = evaluate(pd.DataFrame(data=meanTestSet,columns=['class']), predictions)\n",
    "'''\n",
    "\n",
    "classifier = DecisionTreeClassifier(class_weight={'pos':.983, 'neg':.017},max_depth =3)  \n",
    "classifier.fit(pd.DataFrame(data=mostFrequentTrainSet,columns=COLUMNS), pd.DataFrame(data=mostFrequentTrainSet,columns=['class']))  \n",
    "predictions = classifier.predict(pd.DataFrame(data=mostFrequentTestSet,columns=COLUMNS))  \n",
    "score = evaluate(pd.DataFrame(data=mostFrequentTestSet,columns=['class']), predictions)\n",
    "\n",
    "plot_roc(mostFrequentTestSet['class'], predictions)\n",
    "\n",
    "'''\n",
    "#Hisotgram mean as feature\n",
    "classifier = DecisionTreeClassifier()#class_weight={'pos':.983, 'neg':.017},max_depth =max_depth, min_samples_leaf =min_samples_leaf)  \n",
    "classifier.fit(pd.DataFrame(data=meanTrainSetHist,columns=HISTOGRAMMERGEDCOLUMNS), pd.DataFrame(data=meanTrainSetHist,columns=['class']))  \n",
    "predictions = classifier.predict(pd.DataFrame(data=meanTestSetHist,columns=HISTOGRAMMERGEDCOLUMNS))  \n",
    "score = evaluate(pd.DataFrame(data=meanTestSetHist,columns=['class']), predictions)\n",
    "\n",
    "\n",
    "classifier = DecisionTreeClassifier()#class_weight={'pos':.983, 'neg':.017},max_depth =max_depth, min_samples_leaf =min_samples_leaf)  \n",
    "classifier.fit(pd.DataFrame(data=medianTrainSetHist,columns=HISTOGRAMMERGEDCOLUMNS), pd.DataFrame(data=medianTrainSetHist,columns=['class']))  \n",
    "predictions = classifier.predict(pd.DataFrame(data=medianTestSetHist,columns=HISTOGRAMMERGEDCOLUMNS))  \n",
    "score = evaluate(pd.DataFrame(data=medianTestSetHist,columns=['class']), predictions)\n",
    "\n",
    "classifier = DecisionTreeClassifier(class_weight={'pos':.983, 'neg':.017},max_depth =max_depth, min_samples_leaf =min_samples_leaf)  \n",
    "classifier.fit(pd.DataFrame(data=mostFrequentTrainSetHist,columns=HISTOGRAMMERGEDCOLUMNS), pd.DataFrame(data=mostFrequentTrainSetHist,columns=['class']))  \n",
    "predictions = classifier.predict(pd.DataFrame(data=mostFrequentTestSetHist,columns=HISTOGRAMMERGEDCOLUMNS))  \n",
    "score = evaluate(pd.DataFrame(data=mostFrequentTestSetHist,columns=['class']), predictions)\n",
    "\n",
    "\n",
    "#Printing of the decision tree\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None, class_names=CLASS_LABELS, feature_names=COLUMNS, \n",
    "                         filled=True, rounded=True,) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"DT\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of scikit Support vector Machine(Linear)\n",
    "#################################################################################\n",
    "linearSvm = svm.LinearSVC(class_weight={'pos':.983, 'neg':.017})\n",
    "\n",
    "target = pd.DataFrame(data=mostFrequentTrainSet,columns=['class'])\n",
    "\n",
    "linearSvm.fit(pd.DataFrame(data=mostFrequentTrainSet,columns=COLUMNS),target.values.ravel())  \n",
    "predictions = linearSvm.predict(pd.DataFrame(data=mostFrequentTestSet,columns=COLUMNS))  \n",
    "score = evaluate(pd.DataFrame(data=mostFrequentTestSet,columns=['class']), predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
