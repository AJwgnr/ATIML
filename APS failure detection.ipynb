{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Open Points:\n",
    "- How to deal with histogram data -> Currently each bin is seen as a feature\n",
    "- Try different feature selection/reduction techniques\n",
    "- Try different classification algorithms\n",
    "- include unbalancedness of classes in algorithm\n",
    "- Implement plotting of results as charts (http://scikit-learn.org/stable/modules/learning_curve.html)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import precision_score\n",
    "import graphviz \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "#Data reaading parameter\n",
    "TEST_PATH ='dataset/aps_failure_test_set.csv'\n",
    "TRAIN_PATH ='dataset/aps_failure_training_set.csv'\n",
    "COLUMNS = ['aa_000', 'ab_000', 'ac_000', 'ad_000', 'ae_000', 'af_000', 'ag_000', 'ag_001', 'ag_002', 'ag_003', 'ag_004', 'ag_005', 'ag_006', 'ag_007', 'ag_008', 'ag_009', 'ah_000', 'ai_000', 'aj_000', 'ak_000', 'al_000', 'am_0', 'an_000', 'ao_000', 'ap_000', 'aq_000', 'ar_000', 'as_000', 'at_000', 'au_000', 'av_000', 'ax_000', 'ay_000', 'ay_001', 'ay_002', 'ay_003', 'ay_004', 'ay_005', 'ay_006', 'ay_007', 'ay_008', 'ay_009', 'az_000', 'az_001', 'az_002', 'az_003', 'az_004', 'az_005', 'az_006', 'az_007', 'az_008', 'az_009', 'ba_000', 'ba_001', 'ba_002', 'ba_003', 'ba_004', 'ba_005', 'ba_006', 'ba_007', 'ba_008', 'ba_009', 'bb_000', 'bc_000', 'bd_000', 'be_000', 'bf_000', 'bg_000', 'bh_000', 'bi_000', 'bj_000', 'bk_000', 'bl_000', 'bm_000', 'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'bs_000', 'bt_000', 'bu_000', 'bv_000', 'bx_000', 'by_000', 'bz_000', 'ca_000', 'cb_000', 'cc_000', 'cd_000', 'ce_000', 'cf_000', 'cg_000', 'ch_000', 'ci_000', 'cj_000', 'ck_000', 'cl_000', 'cm_000', 'cn_000', 'cn_001', 'cn_002', 'cn_003', 'cn_004', 'cn_005', 'cn_006', 'cn_007', 'cn_008', 'cn_009', 'co_000', 'cp_000', 'cq_000', 'cr_000', 'cs_000', 'cs_001', 'cs_002', 'cs_003', 'cs_004', 'cs_005', 'cs_006', 'cs_007', 'cs_008', 'cs_009', 'ct_000', 'cu_000', 'cv_000', 'cx_000', 'cy_000', 'cz_000', 'da_000', 'db_000', 'dc_000', 'dd_000', 'de_000', 'df_000', 'dg_000', 'dh_000', 'di_000', 'dj_000', 'dk_000', 'dl_000', 'dm_000', 'dn_000', 'do_000', 'dp_000', 'dq_000', 'dr_000', 'ds_000', 'dt_000', 'du_000', 'dv_000', 'dx_000', 'dy_000', 'dz_000', 'ea_000', 'eb_000', 'ec_00', 'ed_000', 'ee_000', 'ee_001', 'ee_002', 'ee_003', 'ee_004', 'ee_005', 'ee_006', 'ee_007', 'ee_008', 'ee_009', 'ef_000', 'eg_000']\n",
    "SKIPROWS = 20\n",
    "\n",
    "#Data preprocessing parameter\n",
    "NaNSTRATEGY = 'mean' #->'mean', 'median', 'most_frequent'\n",
    "\n",
    "\n",
    "def loadDatasetWithPandas(path,skiprowsNum):\n",
    "    #Reading the raw data from csv file\n",
    "    rawData = pd.read_csv(path,skiprows=skiprowsNum)\n",
    "    #display(rawData)  \n",
    "    #replacing the string indicating missing values with the numpy value for missing values\n",
    "    naNProcessedData = rawData.replace({'na': np.nan}, regex=True)\n",
    "    return naNProcessedData\n",
    "    \n",
    "    \n",
    "def processNaNInDataset(data, strategy):\n",
    "    values = data[list(COLUMNS)].values\n",
    "    imp = Imputer(missing_values='NaN', strategy=strategy, axis=0)\n",
    "    imp = imp.fit(values)\n",
    "    cleanedValues = imp.transform(values)\n",
    "    label = data['class'].values\n",
    "    return cleanedValues, label\n",
    "\n",
    "\n",
    "#processing pipeline (http://scikit-learn.org/stable/modules/pipeline.html#pipeline)\n",
    "def processingPipeline(featureReduction, featureSelector, classifier): \n",
    "    if(featureReduction == None):\n",
    "        if(featureSelector == None):\n",
    "            pipeline = make_pipeline(classifier)\n",
    "        else:\n",
    "            pipeline = make_pipeline(featureSelector,classifier)\n",
    "    else:\n",
    "        pipeline = make_pipeline(featureReduction, featureSelector,classifier)\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def findPipelineParameter(pipe):\n",
    "    #http://scikit-learn.org/stable/modules/grid_search.html\n",
    "    # search over whole parameter space of an estimator ->looks really interesting, but i dont´t have time right now to figure out how it works ;P\n",
    "    None\n",
    "    \n",
    "def testPipeline(pipeline):\n",
    "    #train\n",
    "    pipeline.fit(cleanedTrainValues, labelTrain)\n",
    "    #predict\n",
    "    predictions = pip.predict(cleanedTestValues)\n",
    "    #plot\n",
    "    classificationRep = classification_report(labelTest, predictions)\n",
    "    confusionMatrix = confusion_matrix(labelTest,predictions)\n",
    "    \n",
    "    print(classificationRep)\n",
    "    print(confusionMatrix)\n",
    "    print('Score: ' + str(calculateOverallCostFromConfusionMatrix(confusionMatrix)))\n",
    "    return\n",
    "\n",
    "def calculateOverallCostFromConfusionMatrix(confusionMatrix):\n",
    "    #cost function from description\n",
    "    score = confusionMatrix[0][1] * 500 + 10*confusionMatrix[1][1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data Loading and processing of NaN\n",
    "####################################################################\n",
    "#Load Train Dataset\n",
    "trainData = loadDatasetWithPandas(TRAIN_PATH, SKIPROWS)\n",
    "#Load Test Dataset\n",
    "testData = loadDatasetWithPandas(TEST_PATH, SKIPROWS)\n",
    "\n",
    "#TODO: Test which works best: \"mean\", \"median”, or “most_frequent\"\n",
    "cleanedTrainValues, labelTrain = processNaNInDataset(trainData,NaNSTRATEGY)\n",
    "cleanedTestValues, labelTest = processNaNInDataset(testData,NaNSTRATEGY)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree classifier\n",
    "#####################################################################\n",
    "\n",
    "#Tree parameter\n",
    "max_depth = 5\n",
    "min_samples_leaf = 20\n",
    "#min_samples_split =\n",
    "#class_weight ={0:.1, 1:.9}\n",
    "\n",
    "    \n",
    "decisiontree = tree.DecisionTreeClassifier(max_depth =max_depth, min_samples_leaf =min_samples_leaf)\n",
    "#Feature Selector\n",
    "for i in range(50,100):\n",
    "    print('Number of Features: ' + str(i))\n",
    "    featuresSelected = SelectKBest(chi2, k=i)\n",
    "    #feature reduction\n",
    "\n",
    "    #build pipelinee\n",
    "    pip = processingPipeline(None, featuresSelected,decisiontree)\n",
    "    #evaluate pipeline\n",
    "    testPipeline(pip)\n",
    "\n",
    "#For generating the DT pdf later on\n",
    "#dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "#graph = graphviz.Source(dot_data) \n",
    "#graph.render(\"DT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "#-> Not reproducible results!!!!!\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for  g in range(0,10):\n",
    "    randomfo = RandomForestClassifier(n_estimators=50)\n",
    "    #build pipelinee\n",
    "    pip = processingPipeline(None, None,randomfo)\n",
    "    #evaluate pipeline\n",
    "    testPipeline(pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement SVM's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
